{
 "cells": [
  {
   "attachments": {
    "img92.gif": {
     "image/gif": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Date May 12, 2018 \n",
    "\n",
    "Cross Validation mechanics - a statistical method of evaluating and comparing regression and other learning algorithms by dividing data into complementary segments: at least one used to learn or train a model and another used to validate the model. \n",
    "\n",
    "In typical cross-validation, the training and validation sets must cross over in successive rounds such that each data point has a chance of being validated against.\n",
    "\n",
    "Cross-validation can be applied in three contexts: performance estimation, model selection, and tuning learning model parameters - a type of monitoring technique then, of machine learning models.\n",
    "\n",
    "Pointers and Tips\n",
    " - Train the model for each split\n",
    " - Average the scores\n",
    " - Train model on all the data\n",
    " - Establishes a lower bound score\n",
    " - Important to have a baseline\n",
    "\n",
    "Read more --> http://leitang.net/papers/ency-cross-validation.pdf \n",
    "\n",
    "Nearest Neighbor --> http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms\n",
    "\n",
    "![img92.gif](attachment:img92.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dc_listings = pd.read_csv(\"dc_airbnb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout validation or 2-Fold involves:\n",
    "\n",
    "- splitting the full dataset into 2 partitions or segments: a training set and a test set\n",
    "- training the model on the training set,\n",
    "- using the trained model to predict labels on the test set,\n",
    "- computing an error metric to understand the model's effectiveness,\n",
    "- switch the training and test sets and repeat,\n",
    "- average the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_commas = dc_listings['price'].str.replace(',', '')\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "dc_listings['price'] = stripped_dollars.astype('float')\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "#shuffle\n",
    "s_index = np.random.permutation(dc_listings.shape[0])\n",
    "dc_listings = dc_listings.loc[s_index]\n",
    "\n",
    "split_one = dc_listings.iloc[0:1862]\n",
    "split_two = dc_listings.iloc[1862:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.96254732948216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "#cross validation involves iterating while switching train and test splits\n",
    "train_one = split_one\n",
    "test_one = split_two\n",
    "train_two = split_two\n",
    "test_two = split_one\n",
    "\n",
    "# for documentation on algorithm = auto parameter of KNeighborsRegressor\n",
    "# see http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms\n",
    "# first iteration\n",
    "train_columns = ['accommodates']\n",
    "knn = KNeighborsRegressor(n_neighbors=5, algorithm='auto', metric='euclidean')\n",
    "knn.fit(train_one[train_columns], train_one['price'])\n",
    "prediction1 = knn.predict(test_one[train_columns])\n",
    "\n",
    "# score performance using MSE and RMSE\n",
    "# RMSE is aka residual sum of squares\n",
    "y_true = test_one['price'].as_matrix()\n",
    "y_pred = prediction1\n",
    "iteration_one_mse = mean_squared_error(y_true,y_pred)\n",
    "iteration_one_rmse = iteration_one_mse**0.5    \n",
    "\n",
    "# second iteration\n",
    "train_columns = ['accommodates']\n",
    "knn = KNeighborsRegressor(n_neighbors=5, algorithm='auto', metric='euclidean')\n",
    "knn.fit(train_two[train_columns], train_two['price'])\n",
    "prediction2 = knn.predict(test_two[train_columns])\n",
    "\n",
    "# score performance using MSE and RMSE\n",
    "# RMSE is aka residual sum of squares\n",
    "y_true = test_two['price'].as_matrix()\n",
    "y_pred = prediction2\n",
    "iteration_two_mse = mean_squared_error(y_true,y_pred)\n",
    "iteration_two_rmse = iteration_two_mse**0.5   \n",
    "\n",
    "avg_rmse = np.mean([iteration_one_rmse,iteration_two_rmse])\n",
    "print(avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation \n",
    "\n",
    "K-fold cross validation, on the other hand, takes advantage of a larger proportion of the data during training while still rotating through different segments or subsets of the data to avoid the issues of train/test validation.\n",
    "\n",
    "The algorithm for k-fold cross validation involves:\n",
    "\n",
    "- splitting the full dataset into k equal length partitions.\n",
    "- selecting k-1 partitions as the training set and\n",
    "- selecting the remaining partition as the test set\n",
    "- training the model on the training set.\n",
    "- using the trained model to predict labels on the test fold.\n",
    "- computing the test fold's error metric.\n",
    "- repeating all of the above steps k-1 times, until each partition has been used as the test set for an iteration.\n",
    "- calculating the mean of the k error values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generally 5 folds are used\n",
    "dc_listings.loc[dc_listings.index[0:745], \"fold\"] = 1\n",
    "dc_listings.loc[dc_listings.index[745:1490], \"fold\"] = 2\n",
    "dc_listings.loc[dc_listings.index[1490:2234], \"fold\"] = 3\n",
    "dc_listings.loc[dc_listings.index[2234:2978], \"fold\"] = 4\n",
    "dc_listings.loc[dc_listings.index[2978:3723], \"fold\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.04609155929425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#cross validation involves iterating while switching train and test splits\n",
    "train_split = dc_listings[dc_listings['fold'].isin([2,3,4,5])]\n",
    "test_split = dc_listings[dc_listings['fold'].isin([1])]\n",
    "\n",
    "# first iteration, default neighbors = 5 and is optional\n",
    "# train on the hypothesis that 'accommodates' influences the price \n",
    "train_columns = ['accommodates']\n",
    "knn = KNeighborsRegressor(n_neighbors=5, algorithm='auto', metric='euclidean')\n",
    "knn.fit(train_split[train_columns], train_split['price'])\n",
    "prediction = knn.predict(test_split[train_columns])\n",
    "\n",
    "# score performance using MSE and RMSE\n",
    "# RMSE is aka residual sum of squares\n",
    "y_true = test_split['price'].as_matrix()\n",
    "y_pred = prediction\n",
    "iteration_one_mse = mean_squared_error(y_true,y_pred)\n",
    "iteration_one_rmse = iteration_one_mse**0.5  \n",
    "print(iteration_one_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107.04609155929425, 136.62225078440179, 153.0273362676136, 107.39207160219395, 146.9242838376558]\n",
      "130.20240681023188\n",
      "19.4850676211877\n"
     ]
    }
   ],
   "source": [
    "# Try wrapping the training model in a function\n",
    "# Use np.mean to calculate the mean.\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "fold_ids = [1,2,3,4,5]\n",
    "\n",
    "def train_and_validate(df,folds):\n",
    "    \n",
    "    thermses = []\n",
    "    for f in folds:\n",
    "        train_split = df[df['fold'] != f]\n",
    "        test_split = df[df['fold'] == f]\n",
    "    #train the split\n",
    "        train_columns = ['accommodates']\n",
    "        knn = KNeighborsRegressor(n_neighbors=5, algorithm='auto', metric='euclidean')\n",
    "        knn.fit(train_split[train_columns], train_split['price'])\n",
    "    \n",
    "        prediction = knn.predict(test_split[train_columns])\n",
    "\n",
    "        # performance using MSE and RMSE\n",
    "# RMSE is aka residual sum of squares\n",
    "        y_true = test_split['price'].as_matrix()\n",
    "        y_pred = prediction\n",
    "        iteration_one_mse = mean_squared_error(y_true,y_pred)\n",
    "        iteration_one_rmse = iteration_one_mse**0.5\n",
    "        thermses.append(iteration_one_rmse)\n",
    "    return(thermses)\n",
    "\n",
    "rmses = train_and_validate (dc_listings,fold_ids)\n",
    "avg_rmse = np.mean(rmses)\n",
    "print(rmses)\n",
    "print(avg_rmse)\n",
    "print(np.std(rmses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using KFold and Cross_Val_Score\n",
    "\n",
    "The function written as is, however, has many limitations. If we want to now change the number of folds we want to use, we need to make the function more general so it can also handle randomizing the ordering of the rows in the dataframe and splitting into folds.\n",
    "\n",
    "In machine learning, we're interested in building a good model and accurately understanding how well it will perform. To build a better k-nearest neighbors model, we can change the features it uses or tweak the number of neighbors (a hyperparameter). To accurately understand a model's performance, we can perform k-fold cross validation and select the proper number of folds. The scikit-learn library makes it easy for us to quickly experiment with these different knobs when it comes to building a better model. \n",
    "\n",
    "The KFold class returns an iterator object which we use in conjunction with the cross_val_score() function, also from sklearn.model_selection. Together, these 2 functions allow us to compactly train and test using k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134.66322485283825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "X1 = dc_listings[['accommodates']]\n",
    "y1 = dc_listings[['price']]\n",
    "mses = cross_val_score(estimator=knn, X=X1, y=y1, cv=kf, scoring='neg_mean_squared_error')\n",
    "mses = np.abs(mses)\n",
    "avg_rmse = np.mean(np.sqrt(mses))\n",
    "print(avg_rmse)\n",
    "\n",
    "# compare to above rmse - in a way we are building some type of baseline score for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the right k value \n",
    "\n",
    "Choosing the right k for k-fold cross validation is more of an art and less of a science. A k value of 2 is really just holdout validation. On the other end, setting k equal to n (the number of observations in the data set) is known as leave-one-out cross validation, or LOOCV for short. Through lots of trial and error, data scientists have converged on 10 as the standard k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 folds:  avg RMSE:  126.19223474162338 std RMSE:  1.1069911739514842\n",
      "5 folds:  avg RMSE:  134.66322485283825 std RMSE:  17.06396645051883\n",
      "7 folds:  avg RMSE:  128.76102940687468 std RMSE:  15.113036472055438\n",
      "9 folds:  avg RMSE:  130.97403954965455 std RMSE:  17.552727811846307\n",
      "10 folds:  avg RMSE:  129.38243003886737 std RMSE:  22.858028610253182\n",
      "11 folds:  avg RMSE:  128.5209174821928 std RMSE:  21.039336959245166\n",
      "13 folds:  avg RMSE:  128.66536927916962 std RMSE:  29.931738536000207\n",
      "15 folds:  avg RMSE:  127.74903938013544 std RMSE:  30.22520525014004\n",
      "17 folds:  avg RMSE:  125.08689980063376 std RMSE:  34.7037432777398\n",
      "19 folds:  avg RMSE:  123.24952437277325 std RMSE:  37.93258646028018\n",
      "21 folds:  avg RMSE:  129.74292153412665 std RMSE:  36.31090634328946\n",
      "23 folds:  avg RMSE:  129.91038839859866 std RMSE:  37.34861558876879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "num_folds = [3, 5, 7, 9, 10, 11, 13, 15, 17, 19, 21, 23]\n",
    "\n",
    "for fold in num_folds:\n",
    "    kf = KFold(fold, shuffle=True, random_state=1)\n",
    "    model = KNeighborsRegressor()\n",
    "    mses = cross_val_score(model, dc_listings[[\"accommodates\"]], dc_listings[\"price\"], scoring=\"neg_mean_squared_error\", cv=kf)\n",
    "    rmses = np.sqrt(np.absolute(mses))\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    std_rmse = np.std(rmses)\n",
    "    print(str(fold), \"folds: \", \"avg RMSE: \", str(avg_rmse), \"std RMSE: \", str(std_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance\n",
    "\n",
    "Parting thoughts: a lower RMSE does NOT always mean that a model is more accurate. A model has two other sources of error, bias and variance.\n",
    "\n",
    "Bias describes error that results in bad assumptions about the learning algorithm. For example, assuming that only one feature, like a car's weight, relates to a car's fuel efficiency will lead you to fit a simple, univariate regression model that will result in high bias. The error rate will be high since a car's fuel efficiency is affected by many other factors besides just its weight.\n",
    "\n",
    "Variance describes error that occurs because of the variability of a model's predicted values. If we were given a dataset with 1000 features on each car and used every single feature to train an incredibly complicated multivariate regression model, we will have low bias but high variance. In an ideal world, we want low bias and low variance but in reality, there's always a tradeoff.\n",
    "\n",
    "The standard deviation of the RMSE values can be a proxy for a model's variance while the average RMSE is a proxy for a model's bias. Bias and variance are the 2 observable sources of error in a model that we can indirectly control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
